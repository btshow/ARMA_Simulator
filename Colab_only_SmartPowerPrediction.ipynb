{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab_only_SmartPowerPrediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM0msP6U+fWcW7LUilHTypW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/btshow/ARMA_Simulator/blob/main/Colab_only_SmartPowerPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZmY0-nHOocS",
        "outputId": "c6bfbbad-7cf0-4527-f890-cabecc48bfbe"
      },
      "source": [
        "!apt-get install python3.6\n",
        "!python --version"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python3.6 is already the newest version (3.6.9-1~18.04ubuntu1.3).\n",
            "python3.6 set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 10 not upgraded.\n",
            "Python 3.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nu3BMElxMoy0",
        "outputId": "0e7667e4-bb6e-4ef4-a073-c4a9f40966dc"
      },
      "source": [
        "!git clone https://github.com/GoogleCloudPlatform/professional-services.git\n",
        "!pip install pandas==0.23.4\n",
        "!pip install scikit-learn==0.20.2\n",
        "%tensorflow_version 1.14.0\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import argparse\n",
        "import io\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn.metrics\n",
        "#import trainer.model as model\n",
        "import tensorflow as tf\n",
        "from google.cloud import storage\n",
        "from tensorflow import keras\n",
        "\n",
        "print('tf version is', tf.__version__)\n",
        "print('pandas version is', pd.__version__)\n",
        "\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'professional-services' already exists and is not an empty directory.\n",
            "Requirement already satisfied: pandas==0.23.4 in /usr/local/lib/python3.6/dist-packages (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.4) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.4) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.4) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas==0.23.4) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn==0.20.2 in /usr/local/lib/python3.6/dist-packages (0.20.2)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.20.2) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.20.2) (1.19.5)\n",
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.14.0`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n",
            "tf version is 1.15.2\n",
            "pandas version is 0.23.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "wD4Xo_cXlHqf",
        "outputId": "d8649bb2-934d-4789-aab0-4a2c8a0c51f4"
      },
      "source": [
        "\n",
        "'''\n",
        "#official requirements\n",
        "REQUIRED_PACKAGES = [\n",
        "  \"tensorflow==1.12.0\",\n",
        "  \"google-cloud-storage==1.13.2\",\n",
        "  \"pandas==0.23.4\",\n",
        "  \"scikit-learn==0.20.2\",\n",
        "]\n",
        "#pythonVersion: \"3.5\"\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n#official requirements\\nREQUIRED_PACKAGES = [\\n  \"tensorflow==1.12.0\",\\n  \"google-cloud-storage==1.13.2\",\\n  \"pandas==0.23.4\",\\n  \"scikit-learn==0.20.2\",\\n]\\n\\n#pythonVersion: \"3.5\"\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1rmQFH7cYG6"
      },
      "source": [
        "# Access to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8JhrcjSVw0L"
      },
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOVaFGyGcdcZ"
      },
      "source": [
        "# Load csv file from Google Drive into Colab VM and as a pandas df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z8dCU5BfBRY"
      },
      "source": [
        "\n",
        "def load_csv(link, name):\n",
        "  '''\n",
        "   returns a pandas df corresponding to a csv loaded from google drive \n",
        "   as a side effect loads a csv file named <name> into colab VM\n",
        "  link: string, containing a google drive shareable link (obtained from google drive right-click on file)\n",
        "    e.g. 'https://drive.google.com/file/d/1GNR378iZTsGjiWwJxvb_sCug8IDVcDSS/view?usp=sharing'\n",
        "  name: string, containing how the new csv file should be named after loading into\n",
        "   the colab VM \n",
        "    e.g. 'eval.csv'\n",
        "'''\n",
        "  fluff, id = link.split('d/')\n",
        "  id, fluff = id.split('/view')\n",
        "  downloaded = drive.CreateFile({'id':id}) \n",
        "  return downloaded.GetContentFile(name), pd.read_csv(name)  \n",
        "\n",
        "# eval.csv\n",
        "_, df_eval =load_csv(\n",
        "    'https://drive.google.com/file/d/1GNR378iZTsGjiWwJxvb_sCug8IDVcDSS/view?usp=sharing',\n",
        "    'eval.csv'\n",
        "  )\n",
        "\n",
        "# train.csv\n",
        "_, df_train =load_csv(\n",
        "    'https://drive.google.com/file/d/1OobRyJFXWDSF4MOk4bnloYKF-UVJJuyj/view?usp=sharing',\n",
        "    'train.csv'\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OePVnhvTWTh",
        "outputId": "20b2a72d-532f-48b7-fbd7-a7e87174275d"
      },
      "source": [
        "pd.set_option('max_columns', None)\n",
        "print(df_train.head())\n",
        "print(df_train.columns)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                  time  running_machine  laptop2  washing_machine  \\\n",
            "0  2013-06-01 00:00:01                1        0                3   \n",
            "1  2013-06-01 00:00:07                1        0                3   \n",
            "2  2013-06-01 00:00:13                1        0                3   \n",
            "3  2013-06-01 00:00:19                1        0                3   \n",
            "4  2013-06-01 00:00:25                1        0                3   \n",
            "\n",
            "   dish_washer  fridge  microwave  toaster  playstation  modem  cooker  \\\n",
            "0            0      84          0        1            0      9       0   \n",
            "1            1      83          0        1            0      9       0   \n",
            "2            1      83          0        1            0      9       0   \n",
            "3            1      84          0        1            0      9       0   \n",
            "4            0      83          0        1            0      9       0   \n",
            "\n",
            "   laptop  monitor  speakers  server  router  server_hdd  kettle  rice_cooker  \\\n",
            "0      15       61        10      13       6           0       1            1   \n",
            "1      26       61        10      14       6           0       1            1   \n",
            "2      16       61        11      14       7           0       1            1   \n",
            "3      21       61        10      14       7           1       1            1   \n",
            "4      16       60        10      14       6           1       1            1   \n",
            "\n",
            "   gross  app_sum  running_machine_on  washing_machine_on  dish_washer_on  \\\n",
            "0    205        7                   0                   0               0   \n",
            "1    217        8                   0                   0               0   \n",
            "2    209        8                   0                   0               0   \n",
            "3    215        8                   0                   0               0   \n",
            "4    206        7                   0                   0               0   \n",
            "\n",
            "   microwave_on  toaster_on  kettle_on  rice_cooker_on  cooker_on  \n",
            "0             0           0          0               0          0  \n",
            "1             0           0          0               0          0  \n",
            "2             0           0          0               0          0  \n",
            "3             0           0          0               0          0  \n",
            "4             0           0          0               0          0  \n",
            "Index(['time', 'running_machine', 'laptop2', 'washing_machine', 'dish_washer',\n",
            "       'fridge', 'microwave', 'toaster', 'playstation', 'modem', 'cooker',\n",
            "       'laptop', 'monitor', 'speakers', 'server', 'router', 'server_hdd',\n",
            "       'kettle', 'rice_cooker', 'gross', 'app_sum', 'running_machine_on',\n",
            "       'washing_machine_on', 'dish_washer_on', 'microwave_on', 'toaster_on',\n",
            "       'kettle_on', 'rice_cooker_on', 'cooker_on'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eU7-3bttuhNs"
      },
      "source": [
        "# Specify Data Iterator Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_i9csUJi_EG"
      },
      "source": [
        "def json_serving_input_fn(feat_names):\n",
        "  \"\"\"\n",
        "  Build the serving inputs\n",
        "\n",
        "  Args:\n",
        "    feat_name   - list, Name list of features used in the prediction model.\n",
        "\n",
        "  Returns:\n",
        "    tf.estimator.export.ServingInputReceive\n",
        "  \"\"\"\n",
        "\n",
        "  def serving_input_fn():\n",
        "    feat_cols = [tf.feature_column.numeric_column(x) for x in feat_names]\n",
        "    inputs = {}\n",
        "    for feat in feat_cols:\n",
        "      inputs[feat.name] = tf.placeholder(shape=[None], dtype=feat.dtype)\n",
        "    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n",
        "\n",
        "  return serving_input_fn\n",
        "\n",
        "\n",
        "def make_input_fn(data_file,\n",
        "                  seq_len,\n",
        "                  batch_size,\n",
        "                  cols=None,\n",
        "                  num_epochs=None,\n",
        "                  shuffle=False,\n",
        "                  train_flag=False,\n",
        "                  filter_prob=1.0):\n",
        "  \"\"\"Input function for estimator.\n",
        "\n",
        "  Input function for estimator.\n",
        "\n",
        "  Args:\n",
        "    data_file   - string, Path to input csv file.\n",
        "    seq_len     - int, Length of time sequence.\n",
        "    batch_size  - int, Mini-batch size.\n",
        "    num_epochs  - int, Number of epochs\n",
        "    shuffle     - bool, Whether to shuffle the data\n",
        "    cols        - list, Columns to extract from csv file.\n",
        "    train_flag  - bool, Whether in the training phase, we want to\n",
        "                  ignore sequences when all appliances are off.\n",
        "    filter_prob - float, The probability to pass data sequences with all\n",
        "                  appliances being 'off', only valid when train_flag is True.\n",
        "  Returns:\n",
        "    tf.data.Iterator.\n",
        "  \"\"\"\n",
        "\n",
        "  def _mk_data(*argv):\n",
        "    \"\"\"Format data for further processing.\n",
        "\n",
        "    This function slices data into subsequences, extracts the flags\n",
        "    from the last time steps and treat each as the target for the subsequences.\n",
        "    \"\"\"\n",
        "    data = {'ActivePower_{}'.format(i + 1): x\n",
        "            for i, x in enumerate(tf.split(argv[0], seq_len))}\n",
        "    # Only take the label of the last time step in the sequence as target\n",
        "    flags = [tf.split(x, seq_len)[-1][0] for x in argv[1:]]\n",
        "\n",
        "    #return 2 elements: \n",
        "    # data i.e. the gross electricty consumption series as well as\n",
        "    # a tensor for each input sequence with the labels stacked into one 1-d tensor\n",
        "    # as well as cast into the unit8 dtype format\n",
        "    return data, tf.cast(tf.stack(flags), dtype=tf.uint8)\n",
        "\n",
        "  def _filter_data(data, labels):\n",
        "    \"\"\"Filter those sequences with all appliances 'off'.\n",
        "\n",
        "    Filter those sequences with all appliances 'off'.\n",
        "    However, with filter_prob we pass the sequence.\n",
        "    \"\"\"\n",
        "    rand_num = tf.random_uniform([], 0, 1, dtype=tf.float64)\n",
        "    thresh = tf.constant(filter_prob, dtype=tf.float64, shape=[])\n",
        "    is_all_zero = tf.equal(tf.reduce_sum(labels), 0)\n",
        "    return tf.logical_or(tf.logical_not(is_all_zero),\n",
        "                         tf.less(rand_num, thresh))\n",
        "  ## \n",
        "  '''\n",
        "  From here down: A iterator is created that can be used to feed training data\n",
        "  iteratively to the training algorithm during model training\n",
        "  The iterator imposes the data formats provided in record_defaults on the csv\n",
        "  file read in subsequently\n",
        "  The iterator uses a sliding window of size seq_len to split the time series in \n",
        "  several overlapping! time series of each size seq_len and feeds these \n",
        "  new time series as an iterator i.e. as a DatasetV1Adapter Object\n",
        "  Using _mk_data, the iterator splits each sequence into a 1d tensor of \n",
        "  [seq length] elements a and 1d tensor of 0-1 labels for each sequence \n",
        "  (corresponding to the on-/off status off the appliance during the last \n",
        "  sequence element over all appliances)\n",
        "\n",
        "  The filter function is optional and only useful when we want to reduce the \n",
        "  number of sequences with all appliances turned of\n",
        "\n",
        "  The other function modify the DatasetV1Adapter Object in order \n",
        "  to shuffle the whole set of sequences before using it further, \n",
        "  to repeat the full batch a number of epochs\n",
        "  to define a size for each minibatch\n",
        "  Finally an tensorflow.python.data.ops.iterator_ops.IteratorV2 iterator object\n",
        "  is created using the DatasetV1Adapter Object dataset.\n",
        "  The iterator object can now be used to feed an ML model \n",
        "\n",
        "\n",
        "  IMPORTANT: To create an iterator for a gross electricty consumption forecasting \n",
        "  model, _mk_data() has to be modified - particularly the line \n",
        "  flags = [tf.split(x, seq_len)[-1][0] for x in argv[1:]]\n",
        "  here has to modified, such that the last element of the electricity consumption \n",
        "  series is used as target variable instead of argv[1:]\n",
        "  Hence, this would be worth a try to generate the forecasting target variable\n",
        "  flags = [tf.split(x, seq_len)[-1][0] for x in argv[0]]\n",
        "\n",
        "  \n",
        "  def _mk_data(*argv):\n",
        "    \"\"\"Format data for further processing.\n",
        "\n",
        "    This function slices data into subsequences, extracts the flags\n",
        "    from the last time steps and treat each as the target for the subsequences.\n",
        "    \"\"\"\n",
        "    data = {'ActivePower_{}'.format(i + 1): x\n",
        "            for i, x in enumerate(tf.split(argv[0], seq_len))}\n",
        "    # Only take the label of the last time step in the sequence as target\n",
        "    flags = [tf.split(x, seq_len)[-1][0] for x in argv[1:]]\n",
        "    return data, tf.cast(tf.stack(flags), dtype=tf.uint8)  \n",
        "  '''\n",
        "  \n",
        "  # create the list record_defaults (a list of tf DType objects e.g. tf.float64)\n",
        "  record_defaults = [tf.float64, ] + [tf.int32] * (len(cols) - 1)\n",
        "\n",
        "  #create the  'tensorflow.contrib.data.python.ops.readers.CsvDataset'-object dataset\n",
        "  dataset = tf.contrib.data.CsvDataset([data_file, ],\n",
        "                                       record_defaults,\n",
        "                                       header=True,\n",
        "                                       select_cols=cols)\n",
        "  # create the 'tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter'-object dataset\n",
        "  dataset = dataset.apply(\n",
        "    tf.contrib.data.sliding_window_batch(window_size=seq_len))\n",
        "  \n",
        "  # applies _mk_data to each element of dataset i.e. to each sliding\n",
        "  dataset = dataset.map(_mk_data)\n",
        "\n",
        "  if train_flag:\n",
        "    dataset = dataset.filter(_filter_data).shuffle(60 * 60 * 24 * 7)\n",
        "\n",
        "  if shuffle:\n",
        "    dataset = dataset.shuffle(buffer_size=batch_size * 10)\n",
        "\n",
        "  dataset = dataset.repeat(num_epochs)\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.prefetch(buffer_size=1)\n",
        "\n",
        "  iterator = dataset.make_one_shot_iterator()\n",
        "  return iterator.get_next()\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MofK7tuKyEFN"
      },
      "source": [
        "# Specify ML Model and its Outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uxTcBX4yQYA"
      },
      "source": [
        "\n",
        "  Denote gross energy in the house as a sequence\n",
        "  $(x_t, x_{t+1}, \\cdots, x_{t+n-1}) \\in \\mathcal{R}^n$\n",
        "\n",
        "  Denote the on/off states of appliances at time $t$ as\n",
        "  $y_{t} = \\{y^i_t \\mid y^i_t = [{appliance}\\ i\\ is\\ on\\ at\\ time\\ t ]\\}$,\n",
        "  \n",
        "  We are learning the function\n",
        "  $f(x_t, x_{t+1}, \\cdots, x_{t+n-1}) \\mapsto \\hat{y}_{t+n-1}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4cOGxkJVI0c"
      },
      "source": [
        "def model_fn(features, labels, mode, params):\n",
        "  \"\"\"Build a customized model for energy disaggregation.\n",
        "\n",
        "  The model authoring uses pure tensorflow.layers.\n",
        "\n",
        "  Args:\n",
        "    features: dict(str, tf.data.Dataset)\n",
        "    labels: tf.data.Dataset\n",
        "    mode: One of {tf.estimator.ModeKeys.EVAL,\n",
        "                  tf.estimator.ModeKeys.TRAIN,\n",
        "                  tf.estimator.ModeKeys.PREDICT}\n",
        "    params: Other related parameters\n",
        "\n",
        "  Returns:\n",
        "    tf.estimator.EstimatorSpec.\n",
        "  \"\"\"\n",
        "\n",
        "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "    tf.logging.info('TRAIN')\n",
        "  else:\n",
        "    tf.logging.info('EVAL | PREDICT')\n",
        "\n",
        "  feat_cols = [tf.feature_column.numeric_column(x) for x in params['feat_cols']]\n",
        "  seq_data = tf.feature_column.input_layer(features, feat_cols)\n",
        "\n",
        "  if not params['use_keras']:\n",
        "    tf.logging.info('Tensorflow authoring')\n",
        "\n",
        "    seq_data_shape = tf.shape(seq_data)\n",
        "    batch_size = seq_data_shape[0]\n",
        "\n",
        "    # RNN network using multilayer LSTM\n",
        "\n",
        "    #dropout in LSTM layer with 'lstm_size' units\n",
        "    cells = [tf.nn.rnn_cell.DropoutWrapper(\n",
        "      tf.nn.rnn_cell.LSTMCell(params['lstm_size']), input_keep_prob=1 - params['dropout_rate'])\n",
        "      for _ in range(params['num_layers'])]\n",
        "\n",
        "    #  Compose lstm (ie \"RNN\") cell composed sequentially of multiple cells given by \"cells\"\n",
        "    lstm = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
        "\n",
        "    # Initialize the state of each LSTM cell to zero\n",
        "    state = lstm.zero_state(batch_size, dtype=tf.float32)\n",
        "\n",
        "    # Unroll multiple time steps and the output size is:\n",
        "    # [batch_size, max_time, cell.output_size]\n",
        "    outputs, states = tf.nn.dynamic_rnn(cell=lstm,\n",
        "                                        inputs=tf.expand_dims(seq_data, -1),\n",
        "                                        initial_state=state,\n",
        "                                        dtype=tf.float32)\n",
        "\n",
        "    # Flatten the 3D output to 2D as [batch_size, max_time * cell.output_size]\n",
        "    flatten_outputs = tf.layers.Flatten()(outputs)\n",
        "\n",
        "    # A fully connected layer. The number of output equals the number of target appliances\n",
        "    logits = tf.layers.Dense(params['num_appliances'])(flatten_outputs)\n",
        "\n",
        "  else:\n",
        "    tf.logging.info('Keras authoring')\n",
        "\n",
        "####### RNN Model code zu  sequence2sequence anpassen #######\n",
        "    # RNN network using multilayer LSTM with the help of Keras\n",
        "    model = keras.Sequential()\n",
        "    for _ in range(params['num_layers']):\n",
        "      model.add(\n",
        "        keras.layers.LSTM(params['lstm_size'],\n",
        "                          dropout=params['dropout_rate'],\n",
        "                          return_sequences=True)\n",
        "      )\n",
        "\n",
        "    # Flatten the 3D output to 2D as [batch_size, max_time * cell.output_size]\n",
        "    model.add(keras.layers.Flatten())\n",
        "    # A fully connected layer. The number of output equals the number of target appliances\n",
        "    model.add(keras.layers.Dense(params['num_appliances']))\n",
        "\n",
        "\n",
        "    # Logits can be easily computed using Keras functional API\n",
        "    logits = model(tf.expand_dims(seq_data, -1))\n",
        "\n",
        "####### RNN Output code zu sequence2sequence anpassen (nicht probs) #######\n",
        "  # Probability of turning-on of each appliances corresponding output are computed by applying a sigmoid function on logits\n",
        "  probs = tf.nn.sigmoid(logits)\n",
        "  predictions = {\n",
        "    'probabilities': probs,\n",
        "    'logits': logits\n",
        "  }\n",
        "\n",
        "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "    return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "\n",
        "####### Loss function code zu sequence2sequence loss anpassen (nicht sigmoid_crossent) #######\n",
        "  # Binary cross entropy is used as loss function\n",
        "  loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=labels, logits=logits)\n",
        "  loss_avg = tf.reduce_mean(loss) # kann hier mean loss bleiben. wenn nicht muss loss_avg  evtl. bei train_op auch angepasst werden\n",
        "\n",
        "####### Predictions code zu sequence2sequence anpassen (nicht predicted classes) #######\n",
        "  # Predicted Classses\n",
        "  predicted_classes = tf.cast(tf.round(probs), tf.uint8)\n",
        "\n",
        "####### Metrics code zu sequence2sequence anpassen (nicht precision usw) #######\n",
        "  #Evaluation Metrics\n",
        "  precision = tf.metrics.precision(labels=labels,\n",
        "                                   predictions=predicted_classes)\n",
        "  \n",
        "  recall = tf.metrics.recall(labels=labels,\n",
        "                             predictions=predicted_classes)\n",
        "  f1_score = tf.contrib.metrics.f1_score(labels=labels,\n",
        "                                         predictions=predicted_classes)\n",
        "\n",
        "  metrics = {'precision': precision,\n",
        "             'recall': recall,\n",
        "             'f_measure': f1_score}\n",
        "\n",
        "\n",
        "  if mode == tf.estimator.ModeKeys.EVAL:\n",
        "    return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n",
        "\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=params['learning_rate'])\n",
        "  train_op = optimizer.minimize(loss_avg, global_step=tf.train.get_global_step()) # hier evtl anpassen wenn nicht loss_avg\n",
        "  return tf.estimator.EstimatorSpec(mode,\n",
        "                                    loss=loss,\n",
        "                                    train_op=train_op)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJCTT1N8Kj3n"
      },
      "source": [
        "## Specify Model Hyperparameters and Training Specification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kse_s1AvqCe4",
        "outputId": "c04f86b3-4165-4f09-8a0c-e1910c88df06"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "  ''' \n",
        "  Set Arguments to be passed to the ML model\n",
        "\n",
        "  MAIN OUTPUT from here and used in \"ML Experiment\" is  args\n",
        "\n",
        "  args contains all hyperparamter and model training specifications\n",
        "\n",
        "  After this cell you can run the \n",
        "  '''\n",
        "\n",
        "  parser = argparse.ArgumentParser()\n",
        "  # Input Arguments\n",
        "  parser.add_argument(\n",
        "    '--train-file',\n",
        "    help='GCS or local paths to training data',\n",
        "    default='train.csv'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "    '--num-epochs',\n",
        "    help=\"\"\"\n",
        "      Maximum number of training data epochs on which to train.\n",
        "      If both --max-steps and --num-epochs are specified,\n",
        "      the training job will run for --max-steps or --num-epochs,\n",
        "      whichever occurs first. If unspecified will run for --max-steps.\n",
        "      \"\"\",\n",
        "    type=int,\n",
        "    default=2 #originally = 40\n",
        "  )\n",
        "  parser.add_argument(\n",
        "    '--train-batch-size',\n",
        "    help='Batch size for training steps',\n",
        "    type=int,\n",
        "    default=64\n",
        "  )\n",
        "  parser.add_argument(\n",
        "    '--eval-file',\n",
        "    help='GCS or local paths to evaluation data',\n",
        "    default='eval.csv'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "    '--eval-batch-size',\n",
        "    help='Batch size for evaluation steps',\n",
        "    type=int,\n",
        "    default=64\n",
        "  )\n",
        "  parser.add_argument(\n",
        "    '--test-file',\n",
        "    help='GCS or local paths to test data',\n",
        "  )\n",
        "  # Training arguments\n",
        "  parser.add_argument(\n",
        "    '--seq-len',\n",
        "    help='Length of cropped sequence',\n",
        "    default=5,\n",
        "    type=int\n",
        "  )\n",
        "  parser.add_argument(\n",
        "    '--lstm-size',\n",
        "    help='Size of lstm',\n",
        "    default=5, #originally = 131\n",
        "    type=int\n",
        "  )\n",
        "  parser.add_argument(\n",
        "    '--num-layers',\n",
        "    help='Number of layers in the model',\n",
        "    default=1, #originally = 4\n",
        "    type=int\n",
        "  )\n",
        "  parser.add_argument(\n",
        "    '--dropout-rate',\n",
        "    help='The rate of drop out',\n",
        "    default=0.3204,\n",
        "    type=float\n",
        "  )\n",
        "  parser.add_argument(\n",
        "    '--learning-rate',\n",
        "    help='Learning rate',\n",
        "    default=8.1729e-5,\n",
        "    type=float\n",
        "  )\n",
        "  parser.add_argument(\n",
        "    '--filter-prob',\n",
        "    help='Filter probability',\n",
        "    default=0.6827,\n",
        "    type=float\n",
        "  )\n",
        "  parser.add_argument(\n",
        "    '--job-dir',\n",
        "    help='GCS location to write checkpoints and export models',\n",
        "    default='/tmp/estimator'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "    '--model-name',\n",
        "    help='name of the model',\n",
        "    default='estimator'\n",
        "  )\n",
        "  # Argument to turn on all logging\n",
        "  parser.add_argument(\n",
        "    '--verbosity',\n",
        "    choices=[\n",
        "      'DEBUG',\n",
        "      'ERROR',\n",
        "      'FATAL',\n",
        "      'INFO',\n",
        "      'WARN'\n",
        "    ],\n",
        "    default='INFO',\n",
        "  )\n",
        "  # Experiment arguments\n",
        "  parser.add_argument(\n",
        "    '--train-steps',\n",
        "    help=\"\"\"\\\n",
        "      Steps to run the training job for. If --num-epochs is not specified,\n",
        "      this must be. Otherwise the training job will run indefinitely.\\\n",
        "      \"\"\",\n",
        "    default=1e3, #originally = 1e5\n",
        "    type=int\n",
        "  )\n",
        "  parser.add_argument(\n",
        "    '--eval-steps',\n",
        "    help='Number of steps to run evalution for at each checkpoint',\n",
        "    default=1e2, #originally = 1e3\n",
        "    type=int\n",
        "  )\n",
        "  parser.add_argument(\n",
        "    '--test',\n",
        "    help='Whether to test a model',\n",
        "    default=False,\n",
        "    type=bool\n",
        "  )\n",
        "  parser.add_argument(\n",
        "    '--keras',\n",
        "    help='Whether use keras authoring',\n",
        "    action='store_true'\n",
        "  )\n",
        "\n",
        "  args, _ = parser.parse_known_args()\n",
        "\n",
        "  # Set python level verbosity\n",
        "  tf.logging.set_verbosity(args.verbosity)\n",
        "  # Set C++ Graph Execution level verbosity\n",
        "  os.environ['TF_CPP_MIN_LOG_LEVEL'] = str(tf.logging.__dict__[args.verbosity] / 10)\n",
        "\n",
        "  config = args.__dict__\n",
        "  for k, v in config.items():\n",
        "    tf.logging.info('{}: {}'.format(k, v))\n",
        "    \n",
        " "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:train_file: train.csv\n",
            "INFO:tensorflow:num_epochs: 2\n",
            "INFO:tensorflow:train_batch_size: 64\n",
            "INFO:tensorflow:eval_file: eval.csv\n",
            "INFO:tensorflow:eval_batch_size: 64\n",
            "INFO:tensorflow:test_file: None\n",
            "INFO:tensorflow:seq_len: 5\n",
            "INFO:tensorflow:lstm_size: 5\n",
            "INFO:tensorflow:num_layers: 1\n",
            "INFO:tensorflow:dropout_rate: 0.3204\n",
            "INFO:tensorflow:learning_rate: 8.1729e-05\n",
            "INFO:tensorflow:filter_prob: 0.6827\n",
            "INFO:tensorflow:job_dir: /tmp/estimator\n",
            "INFO:tensorflow:model_name: estimator\n",
            "INFO:tensorflow:verbosity: INFO\n",
            "INFO:tensorflow:train_steps: 1000.0\n",
            "INFO:tensorflow:eval_steps: 100.0\n",
            "INFO:tensorflow:test: False\n",
            "INFO:tensorflow:keras: False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up9DXAH-WCWu"
      },
      "source": [
        "# Now  Train and Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYd4fGxDWY87"
      },
      "source": [
        "import argparse\n",
        "import io\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn.metrics\n",
        "#import trainer.model as model\n",
        "import tensorflow as tf\n",
        "from google.cloud import storage"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hQr58gl3Vr5",
        "outputId": "191d07b3-05c7-40d7-cf34-b38212c79710"
      },
      "source": [
        "# use run experiment contents step by step\n",
        "SELECT_COLUMN = [19, ] + list(range(21, 29))\n",
        "hparams = args\n",
        "\n",
        "select_cols = SELECT_COLUMN\n",
        "feat_col_names = ['ActivePower_{}'.format(i + 1)\n",
        "                    for i in range(hparams.seq_len)]\n",
        "\n",
        "\n",
        "train_input = lambda: make_input_fn( ### delete model.\n",
        "  data_file=hparams.train_file, ### potential error, if hparams i.e. args does not tranport the file correctly\n",
        "  seq_len=hparams.seq_len,\n",
        "  batch_size=hparams.train_batch_size,\n",
        "  cols=select_cols,\n",
        "  train_flag=True,\n",
        "  num_epochs=hparams.num_epochs,\n",
        "  filter_prob=hparams.filter_prob)\n",
        "\n",
        "eval_input = lambda: make_input_fn( ### delete model.\n",
        "  data_file=hparams.eval_file, ### potential error, if hparams i.e. args does not tranport the file correctly\n",
        "  seq_len=hparams.seq_len,\n",
        "  batch_size=hparams.eval_batch_size,\n",
        "  cols=select_cols,\n",
        "  num_epochs=1)\n",
        "\n",
        "\n",
        "model_dir = os.path.join(\n",
        "  hparams.job_dir,\n",
        "  json.loads(os.environ.get('TF_CONFIG', '{}'))\n",
        "  .get('task', {}).get('trial', '')\n",
        "  )\n",
        "\n",
        "\n",
        "tf.logging.info('model dir {}'.format(model_dir))\n",
        "\n",
        "\n",
        "  # Experiment running configuration\n",
        "  # Checkpoint is configured to be saved every ten minutes\n",
        "run_config = tf.estimator.RunConfig(save_checkpoints_steps=2500)\n",
        "run_config = run_config.replace(model_dir=model_dir)\n",
        "\n",
        "\n",
        "params = {'feat_cols': feat_col_names,   ## if using hparams aka args does not work i can specify the dict manually here\n",
        "            'seq_len': hparams.seq_len,\n",
        "            'lstm_size': hparams.lstm_size,\n",
        "            'batch_size': hparams.train_batch_size,\n",
        "            'num_appliances': len(select_cols) - 1,\n",
        "            'num_layers': hparams.num_layers,\n",
        "            'learning_rate': hparams.learning_rate,\n",
        "            'dropout_rate': hparams.dropout_rate,\n",
        "            'use_keras': hparams.keras}\n",
        "\n",
        "estimator = tf.estimator.Estimator(model_fn=model_fn, ### delete model.\n",
        "                                     model_dir=model_dir,\n",
        "                                     config=run_config,\n",
        "                                     params=params)\n",
        "\n",
        "\n",
        "  # Set training spec\n",
        "early_stopping = tf.estimator.experimental.stop_if_no_increase_hook(\n",
        "    estimator,\n",
        "    metric_name='f_measure',\n",
        "    max_steps_without_increase=3000,\n",
        "    min_steps=100,\n",
        "    run_every_secs=300)\n",
        "\n",
        "\n",
        "train_spec = tf.estimator.TrainSpec(input_fn=train_input,\n",
        "                                      max_steps=hparams.train_steps,\n",
        "                                      hooks=[early_stopping])\n",
        "\n",
        "\n",
        "# Set serving function, exporter and evaluation spec\n",
        "# The serving function is only applicable for JSON format input\n",
        "serving_function = json_serving_input_fn(feat_names=feat_col_names)\n",
        "\n",
        "exporter = tf.estimator.FinalExporter(name=hparams.model_name,\n",
        "                                        serving_input_receiver_fn=serving_function)\n",
        "\n",
        "eval_spec = tf.estimator.EvalSpec(input_fn=eval_input,\n",
        "                                    steps=None,\n",
        "                                    throttle_secs=120,\n",
        "                                    exporters=[exporter],\n",
        "                                    name='energy-disaggregation-eval')\n",
        "\n",
        "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:model dir /tmp/estimator/\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/estimator/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 2500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f056371df28>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "INFO:tensorflow:Not using Distribute Coordinator.\n",
            "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
            "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 2500 or save_checkpoints_secs None.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-14-b053000bcbce>:132: CsvDataset.__init__ (from tensorflow.contrib.data.python.ops.readers) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.CsvDataset(...)`.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py:507: sliding_window_batch (from tensorflow.contrib.data.python.ops.sliding) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.window(size=window_size, shift=window_shift, stride=window_stride).flat_map(lambda x: x.batch(window_size))` instead.\n",
            "WARNING:tensorflow:Entity <function make_input_fn.<locals>._mk_data at 0x7f0563792b70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <function make_input_fn.<locals>._mk_data at 0x7f0563792b70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <function make_input_fn.<locals>._filter_data at 0x7f0563792c80> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <function make_input_fn.<locals>._filter_data at 0x7f0563792c80> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:From <ipython-input-14-b053000bcbce>:150: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:TRAIN\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/feature_column/feature_column.py:206: NumericColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/feature_column/feature_column.py:2158: NumericColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/feature_column/feature_column.py:207: NumericColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "INFO:tensorflow:Tensorflow authoring\n",
            "WARNING:tensorflow:From <ipython-input-15-57a22e1199d9>:37: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-15-57a22e1199d9>:40: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-15-57a22e1199d9>:50: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/metrics_impl.py:2026: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/estimator/model.ckpt-100\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 100 into /tmp/estimator/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.7593814, step = 101\n",
            "INFO:tensorflow:global_step/sec: 59.9763\n",
            "INFO:tensorflow:loss = 0.7389056, step = 201 (1.670 sec)\n",
            "INFO:tensorflow:global_step/sec: 63.6965\n",
            "INFO:tensorflow:loss = 0.70644003, step = 301 (1.569 sec)\n",
            "INFO:tensorflow:global_step/sec: 66.0379\n",
            "INFO:tensorflow:loss = 0.6929585, step = 401 (1.518 sec)\n",
            "INFO:tensorflow:global_step/sec: 66.7702\n",
            "INFO:tensorflow:loss = 0.6737398, step = 501 (1.500 sec)\n",
            "INFO:tensorflow:global_step/sec: 67.8567\n",
            "INFO:tensorflow:loss = 0.65747416, step = 601 (1.468 sec)\n",
            "INFO:tensorflow:global_step/sec: 65.8848\n",
            "INFO:tensorflow:loss = 0.63939697, step = 701 (1.517 sec)\n",
            "INFO:tensorflow:global_step/sec: 68.0793\n",
            "INFO:tensorflow:loss = 0.49923083, step = 801 (1.469 sec)\n",
            "INFO:tensorflow:global_step/sec: 65.6438\n",
            "INFO:tensorflow:loss = 0.44814616, step = 901 (1.526 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/estimator/model.ckpt.\n",
            "WARNING:tensorflow:Entity <function make_input_fn.<locals>._mk_data at 0x7f055e6aebf8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <function make_input_fn.<locals>._mk_data at 0x7f055e6aebf8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:EVAL | PREDICT\n",
            "INFO:tensorflow:Tensorflow authoring\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2021-02-20T17:50:14Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/estimator/model.ckpt-1000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2021-02-20-17:51:27\n",
            "INFO:tensorflow:Saving dict for global step 1000: f_measure = 0.018778278, global_step = 1000, loss = 0.374534, precision = 0.0048355046, recall = 0.014594658\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: /tmp/estimator/model.ckpt-1000\n",
            "INFO:tensorflow:Performing the final export in the end of training.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:EVAL | PREDICT\n",
            "INFO:tensorflow:Tensorflow authoring\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
            "INFO:tensorflow:Restoring parameters from /tmp/estimator/model.ckpt-1000\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: /tmp/estimator/export/estimator/temp-b'1613843488'/saved_model.pb\n",
            "INFO:tensorflow:Loss for final step: 0.3792197.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'f_measure': 0.018778278,\n",
              "  'global_step': 1000,\n",
              "  'loss': 0.374534,\n",
              "  'precision': 0.0048355046,\n",
              "  'recall': 0.014594658},\n",
              " [b'/tmp/estimator/export/estimator/1613843488'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfYXL-S4WV9p"
      },
      "source": [
        "# Run the training job\n",
        "run_experiment(args)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}